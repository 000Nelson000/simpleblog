---
layout: post
title: Under the hood of Latent Dirichlet Allocation
date:   2015-01-12
categories: articles
tags: [data-science, LDA, R]
comments: true
share: true
---

### LDA for mortals

I've been intrigued by LDA topic models for a few weeks now.  I actually built one before I really knew what I was doing.  That kind of frightens and excites me at the same time.  On one hand, LDA provides rich output which is easy for the humanist researcher to interpret.  On the other hand, shouldn't I know what kind of crazy machine I'm operating that's spitting out results?  I would certainly like to know more than what's provided in documentation of whatever software implementation I'm using, but I don't need necessarily need to be able to prove every last algorithm before I kick the tires and determine it's a worthwhile algorithm.

I echo [Ted Underwood]'s observation that there isn't much bridge material between the hard core computer science articles for us mortals to develop a basic intution.  My goal for this exercise was to fill the gaps in my brain left by theoretical papers and high level LDA articles by developing a rudimentary LDA model from scratch to learn topics.

### Generative, so what?

Most academic papers I encountered begin by describing LDA as a generative model.  That is, a model that can randomly generate observable data (documents).  [Blei, Ng, & Jordan, 2003] outline this process in their seminal paper on the topic:

> LDA assumes the following generative process for each document w in a corpus D:  
1. Choose N ∼ Poisson(ξ).  
2. Choose θ ∼ Dir(α).  
3. For each of the N words w<sub>n</sub>:  
     (a) Choose a topic z<sub>n</sub> ∼ Multinomial(θ).  
     (b) Choose a word w<sub>n</sub>  from p(w<sub>n</sub> | z<sub>n</sub> ,β), a multinomial probability conditioned on the topic z<sub>n</sub>.  

I found this confusing at first.  I was initially mixing up the generative and inference/training algorithms.  They are different, very different.  I already have my documents...why would I want to probabalistically generate new documents with my LDA model?  Short answer, you probably don't.  However, the generative approach lets you do some cool things:  

*  Assign probabilities to just about everything: words, documents and topics.  [Probabilistic Latent Semantic Indexing] (pLSI), the precursor to LDA, pushed some boundaries in information retrieval, but fell short in providing a probabilistic model at the document level.  
*  The ability to calculate probabilities (topic assignments) for previously unseen documents, which is not possible with pLSI.  
*  Model updates.  That is, you can continue training your model with new documents when they come in without repeatedly starting from scratch and re-training on the full corpus.

There is even some new research ([Zhai and Boyd-Graber, 2013]) exploring breeds of LDA that allow for infinite vocabularies -- assigning probabilities to new documents with words outside of the training vocabulary.

### Statistical inference... how we actually train an LDA model

So the goal of LDA is to compute the 2 hidden parameters often commonly denoted as θ and **z**, where θ represents the per document topic assignments and **z** represents per word topic assignments.

So this is posterior distribution we want to compute:
`p(θ,z|w,α,β) = p(θ,z,w|α,β)/p(w|α,β)`

However, when you do some fancy math, it becomes clear that the posterior distribution is intractable.  That is, we're left with integrals that we cannot compute analytically in high dimensional space where numerical approximation gets hairy.  How to handle this issue is the topic of a large swath of the LDA literature.  It seems that most advocate one of two methods:

-  **Variational inference** seems to be the preferred method for getting fast and accurate results in most software implementations.
-  **Markov chain Monte Carlo (sampling)** methods have the benefit of being unbiased and easy understand.  However, it seems MCMCs are losing some steam as they can be computationally inefficient when working with large corpora.

### Building LDA from scratch

###### Purpose

I have no intention for anyone (including myself) to use this code for anything beyond pedagogical purposes.  There are countless ways to optimize the following code.  I simply wanted to put the theory into action as clearly as possible and dispel any myths in my head that LDA is black magic.  I leave the implementation and optimization to the experts.

###### Getting started

Of all the academic papers I read trying to wrap my head around LDA, I found [Streyver & Griffith's Probabilistic Topic Models] paper to be the most helpful for understanding implementation.  [Their 2004 paper, Finding Scientific Topics] that everyone seems to cite provides a more thorough derivation of the Baysesian magic, but I found their following paper most useful for understanding computation. They clearly walk through how to conduct inference using Collapsed Gibbs Sampling. which I translated into R code. 

[Edwin Chen's Introduction to Latent Dirichlet Allocation post] provides an example of this process using Collapsed Gibbs Sampling in plain english which is a good place to start.

I also found some other homegrown R and Python implementations from [Shuyo] and [Matt Hoffman] -- also great resources.

###### So let's code it

I generated a very trivial corpus of 8 documents.  To focus just on the LDA mechanics, I opted for the simplest of R objects: lists and matrices.
```{r}
## Generate a corpus
rawdocs <- c('eat turkey on turkey day holiday',
          'i like to eat cake on holiday',
          'turkey trot race on thanksgiving holiday',
          'snail race the turtle',
          'time travel space race',
          'movie on thanksgiving',
          'movie at air and space museum is cool movie',
          'aspiring movie star')
docs <- strsplit(rawdocs, split=' ', perl=T) # generate a list of documents

## PARAMETERS
K <- 2 # number of topics
alpha <- 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters
eta <- .001 # hyperparameter
iterations <- 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice.
```

```{r}
print(docs)
```


Abstracting the words away - now we have a numbers problem.
```{r}
## Assign WordIDs to each unique word
vocab <- unique(unlist(docs))

## Replace words in documents with wordIDs
for(i in 1:length(docs)) docs[[i]] <- match(docs[[i]], vocab)
```

```{r}
print(docs)
```

Now we need to generate some basic count matrices.  First we randomly assign topics to each word in each document.  Then we create a word-topic count matrix.

```{r}
## 1. Randomly assign topics to words in each doc.  2. Generate word-topic count matrix.
wt <- matrix(0, K, length(vocab)) # initialize word-topic count matrix
ta <- sapply(docs, function(x) rep(0, length(x))) # initialize topic assignment list
for(d in 1:length(docs)){ # for each document
  for(w in 1:length(docs[[d]])){ # for each token in document d
    ta[[d]][w] <- sample(1:K, 1) # randomly assign topic to token w.
    ti <- ta[[d]][w] # topic index
    wi <- docs[[d]][w] # wordID for token w
    wt[ti,wi] <- wt[ti,wi]+1 # update word-topic count matrix     
  }
}
```

```{r}
print(wt) # Word-topic count matrix
print(ta) # Token-topic assignment list
```

Now we generate a document-topic count matrix where the counts correspond to the number of tokens assigned to each topic for each document.
```{r}
dt <- matrix(0, length(docs), K)
for(d in 1:length(docs)){ # for each document d
  for(t in 1:K){ # for each topic t
    dt[d,t] <- sum(ta[[d]]==t) # count tokens in document d assigned to topic t   
  }
}
```

```{r}
print(dt)
```


If following along with [Streyvers and Griffiths], `wt` corresponds to the C<sup>WT</sup> matrix and `dt` corresponds to the C<sup>WT</sup> matrix.  

Now we get to the fun part where LDA starts learning topics.  We will iteratively update the random (bad) topic assignments we populated into `ta` one token at a  time.  

`p_z` is the full-conditional distribution that token w belongs to topic t.  This is really the only "magic" part of the whole LDA learning process far.  To dispel the magic and truly understand why this works read [Griffiths and Streyvers] -- they walk through the derivation of the full-conditional distribution much more elegantly than I could.

```{r}
for(i in 1:iterations){ # for each pass through the corpus
  for(d in 1:length(docs)){ # for each document
    for(w in 1:length(docs[[d]])){ # for each token 
      
      t0 <- ta[[d]][w] # initial topic assignment to token w
      wid <- docs[[d]][w] # wordID of token w
      
      dt[d,t0] <- dt[d,t0]-1 # we don't want to include token w in our document-topic count matrix when sampling for token w
      wt[t0,wid] <- wt[t0,wid]-1 # we don't want to include token w in our word-topic count matrix when sampling for token w
      
      ## UPDATE TOPIC ASSIGNMENT FOR EACH WORD -- COLLAPSED GIBBS SAMPLING MAGIC.  Where the magic happens.
      denom_a <- sum(dt[d,]) + K * alpha # number of tokens in document + number topics * alpha
      denom_b <- rowSums(wt) + length(vocab) * eta # number of tokens in each topic + # of words in vocab * eta
      p_z <- (wt[,wid] + eta) / denom_b * (dt[d,] + alpha) / denom_a # calculating probability word belongs to each topic
      t1 <- sample(1:K, 1, prob=p_z/sum(p_z)) # draw topic for word n from multinomial using probabilities calculated above
      
      ta[[d]][w] <- t1 # update topic assignment list with newly sampled topic for token w.
      dt[d,t1] <- dt[d,t1]+1 # re-increment document-topic matrix with new topic assignment for token w.
      wt[t1,wid] <- wt[t1,wid]+1 #re-increment word-topic matrix with new topic assignment for token w.
    
      if(t0!=t1) print(paste0('doc:', d, ' token:' ,w, ' topic:',t0,'=>',t1)) # examine when topic assignments change
    }
  }
}
```

So now we've finished learning topics.  We just have to analyze them.  `theta` contains normalizes the counts in the document-topic count matrix `dt` to compute the probability that a document belongs to each topic.

```{r}
theta <- (dt+alpha) / rowSums(dt+alpha) # topic probabilities per document
print(theta)
```

`phi` normalizes the word-topic count matrix `wt` to compute the probability of each word belonging to each topic.

```{r}
phi <- (wt + eta) / (rowSums(wt+eta)) # topic probabilities per word
colnames(phi) <- vocab
print(phi)
```











[Ted Underwood]:http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/
[like so]:http://brooksandrew.github.io/simpleblog/articles/new-york-times-api-to-mongodb/
[gensim LDA]:http://radimrehurek.com/gensim/models/ldamodel.html
[Blei, Ng, & Jordan, 2003]:http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf
[Probabilistic Latent Semantic Indexing]:http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis
[Zhai and Boyd-Graber, 2013]:http://jmlr.org/proceedings/papers/v28/zhai13.pdf
[Probabilistic Topic Models]:http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf
[Streyvers and Griffiths]:http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf
[Shuyo]:https://github.com/shuyo/
[Matt Hoffman]:http://www.cs.princeton.edu/~mdhoffma/
[Their 2004 paper, Finding Scientific Topics]:http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf
[Griffiths and Streyvers]:http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf
[Edwin Chen's Introduction to Latent Dirichlet Allocation post]:http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/


