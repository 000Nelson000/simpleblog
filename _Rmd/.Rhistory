ei<-ei+1
cat('.')
})
if(class(tryget)=='try-error') {
print(paste0(ei, ' error - article not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(pause)
}
while(ei<=length(meta)){
if(length(unique(e[(length(e)-2):length(e)]))==1) ei <- ei+1 ## if we tried and failed 3 times, move on
tryget <- try({
post <- parsePost(url=meta[[ei]]$href)
post$jobid <- meta[[ei]]$jobid
post$jobTitle <- meta[[ei]]$jobTitle
post$href <- meta[[ei]]$href
if(length(addtags)>0) for(at in 1:length(addtags)) post[[names(addtags)[at]]] <- addtags[[at]][1]
globalOrLocal(postsAll[[length(postsAll)+1]], post)
ei<-ei+1
cat('.')
## write to database
if(is.null(mongo$dbName)==F) dbInsertDocument(con, mongo$collection, toJSON(post))
})
if(class(tryget)=='try-error') {
print(paste0(ei, ' error - article not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(pause)
}
addtags=list()
while(ei<=length(meta)){
if(length(unique(e[(length(e)-2):length(e)]))==1) ei <- ei+1 ## if we tried and failed 3 times, move on
tryget <- try({
post <- parsePost(url=meta[[ei]]$href)
post$jobid <- meta[[ei]]$jobid
post$jobTitle <- meta[[ei]]$jobTitle
post$href <- meta[[ei]]$href
if(length(addtags)>0) for(at in 1:length(addtags)) post[[names(addtags)[at]]] <- addtags[[at]][1]
globalOrLocal(postsAll[[length(postsAll)+1]], post)
ei<-ei+1
cat('.')
## write to database
if(is.null(mongo$dbName)==F) dbInsertDocument(con, mongo$collection, toJSON(post))
})
if(class(tryget)=='try-error') {
print(paste0(ei, ' error - article not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(pause)
}
saveGlobally=F
globalOrLocal <- ifelse(saveGlobally==T, `<<-`, `<-`)
e <- c(-3,-2,-1)
ei <- 1
while(ei<=length(meta)){
if(length(unique(e[(length(e)-2):length(e)]))==1) ei <- ei+1 ## if we tried and failed 3 times, move on
tryget <- try({
post <- parsePost(url=meta[[ei]]$href)
post$jobid <- meta[[ei]]$jobid
post$jobTitle <- meta[[ei]]$jobTitle
post$href <- meta[[ei]]$href
if(length(addtags)>0) for(at in 1:length(addtags)) post[[names(addtags)[at]]] <- addtags[[at]][1]
globalOrLocal(postsAll[[length(postsAll)+1]], post)
ei<-ei+1
cat('.')
## write to database
if(is.null(mongo$dbName)==F) dbInsertDocument(con, mongo$collection, toJSON(post))
})
if(class(tryget)=='try-error') {
print(paste0(ei, ' error - article not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(pause)
}
i<-3
urlpg <- paste0(url, i)
meta <- getPosts(urlpg)
if(length(setdiff(sapply(meta, '[[', 'jobid'), sapply(metaAll, '[[', 'jobid')))==0) break ## break if there are no new jobids to scrape
if(justUnique==T) meta <- meta[which(!sapply(meta, '[[', 'jobid') %in% setdiff(sapply(metaAll, '[[', 'jobid'), '0'))] #just scrape unique jobids
metaAll <- append(metaAll, meta)
cat(paste0(length(meta), ' posts scraping from listing page ', i))
sapply(meta, '[[', 'jobid'), sapply(metaAll, '[[', 'jobid'))
sapply(meta, '[[', 'jobid'), sapply(metaAll, '[[', 'jobid')))
setdiff(sapply(meta, '[[', 'jobid'), sapply(metaAll, '[[', 'jobid')))
setdiff(sapply(meta, '[[', 'jobid'), sapply(metaAll, '[[', 'jobid'))
sapply(metaAll, '[[', 'jobid')
setdiff(sapply(meta, '[[', 'jobid')
sapply(meta, '[[', 'jobid')
which(!sapply(meta, '[[', 'jobid') %in% setdiff(sapply(metaAll, '[[', 'jobid'), '0'))
meta[which(!sapply(meta, '[[', 'jobid') %in% setdiff(sapply(metaAll, '[[', 'jobid'), '0'))]
length(meta)
if(justUnique==T) meta <- meta[which(!sapply(meta, '[[', 'jobid') %in% setdiff(sapply(metaAll, '[[', 'jobid'), '0'))] #just scrape unique jobids
length(meta)
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique==T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique==T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
sapply(meta, '[[', 'jobid')
setdiff(sapply(metaAll, '[[', 'jobid'), '0'))
setdiff(sapply(metaAll, '[[', 'jobid'), '0')
print(which(sapply(meta, '[[', 'jobid') %in% sapply(metaAll, '[[', 'jobid')))
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique==T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
justUnique==T
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique==T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
rm(list=ls())
source('~/Downloads/functions.R')
mongo=list(dbName='monster', collection='ds', host='ds049181.mongolab.com:49181', username='user1', password='password')
con <- mongoDbConnect(mongo$dbName, mongo$host)
authenticated <- dbAuthenticate(con, username=mongo$username, password=mongo$password)
library('datasets')
stnames <- gsub(' ', '-', state.name)
stabbs <- state.abb
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique==T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
for(i in 12) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
for(i in 10) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
source('~/Downloads/functions.R')
library('datasets')
stnames <- gsub(' ', '-', state.name)
stabbs <- state.abb
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
length(a)
length(sapply(a, '[[', 'jobid')
)
table(sapply(a, '[[', 'jobid')
)
source('~/Downloads/functions.R')
for(i in 5) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:10, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
table(sapply(a, '[[', 'jobid')
)
dbRemoveQuery(con, 'ds', '[]')
source('~/Downloads/functions.R')
rm(a)
library('datasets')
stnames <- gsub(' ', '-', state.name)
stabbs <- state.abb
for(i in 1:50) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:20, saveGlobally=F, mongo=NULL, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
for(i in 1:50) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-engineer_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:20, saveGlobally=F, mongo=mongo, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
rm(list=ls())
dbRemoveQuery(con, 'ds', '[]')
# authenticate with mongoDB
mongo=list(dbName='monster', collection='ds', host='ds049181.mongolab.com:49181', username='user1', password='password')
con <- mongoDbConnect(mongo$dbName, mongo$host)
authenticated <- dbAuthenticate(con, username=mongo$username, password=mongo$password)
source('~/Downloads/functions.R')
url <- 'http://jobsearch.monster.com/search/data-scientist_5?pg='
a <- streamCollect(pg=1:50, saveGlobally=F, mongo=mongo, addtags=list('searchterm'='data scientist', 'scrape_date'=Sys.Date()))
dbRemoveQuery(con, 'ds', '[]')
## by state
library('datasets')
stnames <- gsub(' ', '-', state.name)
stabbs <- state.abb
for(i in 1:50) {
url <- paste0('http://jobsearch.monster.com/search/', stnames[i], '+data-scientist_15?sort=dt.rv.di&where=', stabbs[i], '&pg=')
a <- streamCollect(url=url, pg=1:20, saveGlobally=F, mongo=mongo, justUnique=T, pause=0,
addtags=list('searchterm'='data scientist', 'scrape_date'=as.character(Sys.Date()), 'state'=stabbs[i]))
cat(paste0(stabbs[i], ' complete  \n'))
}
length(unique(sapply(a, '[[', 'jobid')))
800/16
500/16
31*1000
install.packages('mlearning')
library(noncensus)
library('noncensus')
install.packages('noncensus')
library('noncensus')
library(noncensus)
data(zip_codes)
data(counties)
View(zip_codes)
View(counties)
setnames
setNames
df <- mtcars
setNames(df, 'cyl', 'blah')
setNames(df, c('cyl', 'blah'))
8493/67.94
125/100
100/125
5*100/125
dt <- data.table(mtcars)
library('data.table')
dt[,L:=list(mpg)]
dt[,L:=list(list(mpg))]
dt[,L:=list(list(mpg)), by=cyl]
dt[,L:=list(mpg), by=cyl]
dt[,L:=list(unique(mpg)), by=cyl]
dt[,L:=lapply(.SD, list(unique(mpg))), by=cyl]
dt[,L:=list(1,2,3), by=cyl]
dt[,L:=list(1,2,3)]
dt[,L:=lapply(.SD, list(1,2,3))]
list(1,2,3)
dt[,L:=lapply(.SD, list(c(1,2,3)))]
dt[,L:=lapply(.SD, list(c(1,2,3)))]
dt[,L2:=lapply(.SD, list(c(1,2,3)))]
dt
dt <- data.table(mtcars)
dt
dt[,L:=lapply(.SD, list(c(1,2,3)))]
dt[,L:=list(c(1,2,3))]
dt[, L:=lapply(.SD, list(c(1,2,3)))]
dt[, L:=lapply(.SD, list(list(c(1,2,3))))]
dt[, lapply(.SD, list(list(c(1,2,3))))]
dt[, list(list(c(1,2,3)))]
dt[, list(list(c(1,2,3))), by=.EACHI]
dt[, list(list(c(1,2,3)))]
dt[, a:=list(list(c(1,2,3)))]
dt
dt[, a:=list(mpg, cyl)]
dt
dt[, a:=list(unique(mpg)), by=.(cyl, gear)]
dt[, a:=list(list(unique(mpg))), by=.(cyl, gear)]
dt
dt <- data.table(mtcars)
dt[, a:=list(list(unique(mpg))), by=.(cyl, gear)]
dt
dt[1,1]
dt[1,a]
dt[1,a][1]
dt[1,a][2]
dt[1,a][[1]][2]
dt[1,a][[1]][3]
dt <- data.table(mtcars)
dt[, L:=list(list(c(1,2,3)))]
dt[, a:=list(list(unique(mpg))), by=.(cyl, gear)]
dt[1,a][[1]][3]
dt
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
## might need to rebuild knitr to avoid "cairo graphics error
#library('devtools')
#install_github('yihui/knitr')
#library('knitr')
## source of code
#http://chepec.se/blog/2014/07/16/knitr-jekyll.html
#!/usr/bin/env Rscript
options(stringsAsFactors = F)
# inspiration sources:
# http://www.jonzelner.net/jekyll/knitr/r/2014/07/02/autogen-knitr/
# http://gtog.github.io/workflow/2013/06/12/rmarkdown-to-rbloggers/
KnitPost <- function(site.path='/Users/abrooks/Documents/github/simpleblog/', overwriteAll=F, overwriteOne=NULL) {
if(!'package:knitr' %in% search()) library('knitr')
site.path <- site.path # directory of jekyll blog (including trailing slash)
rmd.path <- paste0(site.path, "_Rmd") # directory where your Rmd-files reside (relative to base)
fig.dir <- "assets/Rfig/" # directory to save figures
posts.path <- paste0(site.path, "_posts/articles/") # directory for converted markdown files
cache.path <- paste0(site.path, "_cache") # necessary for plots
render_jekyll(highlight = "pygments")
opts_knit$set(base.url = '/', base.dir = site.path)
opts_chunk$set(fig.path=fig.dir, fig.width=8.5, fig.height=5.25, dev='svg', cache=F,
warning=F, message=F, cache.path=cache.path, tidy=F)
setwd(rmd.path) # setwd to base
# some logic to help us avoid overwriting already existing md files
files.rmd <- data.frame(rmd = list.files(path = rmd.path,
full.names = T,
pattern = "\\.Rmd$",
ignore.case = T,
recursive = F), stringsAsFactors=F)
files.rmd$corresponding.md.file <- paste0(posts.path, "/", basename(gsub(pattern = "\\.Rmd$", replacement = ".md", x = files.rmd$rmd)))
files.rmd$corresponding.md.exists <- file.exists(files.rmd$corresponding.md.file)
## determining which posts to overwrite from parameters overwriteOne & overwriteAll
files.rmd$md.overwriteAll <- overwriteAll
if(is.null(overwriteOne)==F) files.rmd$md.overwriteAll[grep(overwriteOne, files.rmd[,'rmd'], ignore.case=T)] <- T
files.rmd$md.render <- F
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$corresponding.md.exists[i] == F) {
files.rmd$md.render[i] <- T
}
if ((files.rmd$corresponding.md.exists[i] == T) && (files.rmd$md.overwriteAll[i] == T)) {
files.rmd$md.render[i] <- T
}
}
# For each Rmd file, render markdown (contingent on the flags set above)
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$md.render[i] == T) {
out.file <- knit(as.character(files.rmd$rmd[i]),
output = as.character(files.rmd$corresponding.md.file[i]),
envir = parent.frame(),
quiet = T)
message(paste0("KnitPost(): ", basename(files.rmd$rmd[i])))
}
}
}
## actually using function
#KnitPost(overwriteOne='test', site.path='/Users/ajb/Documents/github/simpleblog/')
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
## might need to rebuild knitr to avoid "cairo graphics error
#library('devtools')
#install_github('yihui/knitr')
#library('knitr')
## source of code
#http://chepec.se/blog/2014/07/16/knitr-jekyll.html
#!/usr/bin/env Rscript
options(stringsAsFactors = F)
# inspiration sources:
# http://www.jonzelner.net/jekyll/knitr/r/2014/07/02/autogen-knitr/
# http://gtog.github.io/workflow/2013/06/12/rmarkdown-to-rbloggers/
KnitPost <- function(site.path='/Users/abrooks/Documents/github/simpleblog/', overwriteAll=F, overwriteOne=NULL) {
if(!'package:knitr' %in% search()) library('knitr')
site.path <- site.path # directory of jekyll blog (including trailing slash)
rmd.path <- paste0(site.path, "_Rmd") # directory where your Rmd-files reside (relative to base)
fig.dir <- "assets/Rfig/" # directory to save figures
posts.path <- paste0(site.path, "_posts/articles/") # directory for converted markdown files
cache.path <- paste0(site.path, "_cache") # necessary for plots
render_jekyll(highlight = "pygments")
opts_knit$set(base.url = '/', base.dir = site.path)
opts_chunk$set(fig.path=fig.dir, fig.width=8.5, fig.height=5.25, dev='svg', cache=F,
warning=F, message=F, cache.path=cache.path, tidy=F)
setwd(rmd.path) # setwd to base
# some logic to help us avoid overwriting already existing md files
files.rmd <- data.frame(rmd = list.files(path = rmd.path,
full.names = T,
pattern = "\\.Rmd$",
ignore.case = T,
recursive = F), stringsAsFactors=F)
files.rmd$corresponding.md.file <- paste0(posts.path, "/", basename(gsub(pattern = "\\.Rmd$", replacement = ".md", x = files.rmd$rmd)))
files.rmd$corresponding.md.exists <- file.exists(files.rmd$corresponding.md.file)
## determining which posts to overwrite from parameters overwriteOne & overwriteAll
files.rmd$md.overwriteAll <- overwriteAll
if(is.null(overwriteOne)==F) files.rmd$md.overwriteAll[grep(overwriteOne, files.rmd[,'rmd'], ignore.case=T)] <- T
files.rmd$md.render <- F
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$corresponding.md.exists[i] == F) {
files.rmd$md.render[i] <- T
}
if ((files.rmd$corresponding.md.exists[i] == T) && (files.rmd$md.overwriteAll[i] == T)) {
files.rmd$md.render[i] <- T
}
}
# For each Rmd file, render markdown (contingent on the flags set above)
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$md.render[i] == T) {
out.file <- knit(as.character(files.rmd$rmd[i]),
output = as.character(files.rmd$corresponding.md.file[i]),
envir = parent.frame(),
quiet = T)
message(paste0("KnitPost(): ", basename(files.rmd$rmd[i])))
}
}
}
## actually using function
#KnitPost(overwriteOne='test', site.path='/Users/ajb/Documents/github/simpleblog/')
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
dt <- data.table(mtcars)[,.(mpg, cyl, gear)]
dt[, L:=list(list(c(1,2,3)))]
dt[, a:=list(list(unique(mpg))), by=.(cyl, gear)]
dt[1,a][[1]][3]
dt[,a[[1]]]
dt
dt$a
dt[,lapply(.SD, .SD[1])]
dt[,lapply(.SD, .SD[[1]])]
dt[,lapply(mpg, .SD[[1]])]
dt[,lapply(mpg, .SD[[1,1]])]
dt[,lapply(mpg, function(x,  x[1,1]])]
dt[,lapply(mpg, function(x,  x[1,1]]))]
heaD(dt[,sss:=4])
head(dt[,sss:=4])
dt[,sss:=4]
dt[,sss:=4][]
?"data.table"
install.packages('data.table')
install.packages("data.table")
library('data.table')
?data.table
?data.table
install.packages("Rdatatable/datatable")
install_github("Rdatatable/datatable")
library('devtools')
install_github("Rdatatable/datatable")
library('data.table')
dt[,a[[1]]]
dt[,lapply(mpg, function(x,  x[1,1]]))]
dt[,lapply(mpg, function(x),  x[1,1]]))]
dt[,lapply(mpg, function(x),  x[1,1]])]
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
## might need to rebuild knitr to avoid "cairo graphics error
#library('devtools')
#install_github('yihui/knitr')
#library('knitr')
## source of code
#http://chepec.se/blog/2014/07/16/knitr-jekyll.html
#!/usr/bin/env Rscript
options(stringsAsFactors = F)
# inspiration sources:
# http://www.jonzelner.net/jekyll/knitr/r/2014/07/02/autogen-knitr/
# http://gtog.github.io/workflow/2013/06/12/rmarkdown-to-rbloggers/
KnitPost <- function(site.path='/Users/abrooks/Documents/github/simpleblog/', overwriteAll=F, overwriteOne=NULL) {
if(!'package:knitr' %in% search()) library('knitr')
site.path <- site.path # directory of jekyll blog (including trailing slash)
rmd.path <- paste0(site.path, "_Rmd") # directory where your Rmd-files reside (relative to base)
fig.dir <- "assets/Rfig/" # directory to save figures
posts.path <- paste0(site.path, "_posts/articles/") # directory for converted markdown files
cache.path <- paste0(site.path, "_cache") # necessary for plots
render_jekyll(highlight = "pygments")
opts_knit$set(base.url = '/', base.dir = site.path)
opts_chunk$set(fig.path=fig.dir, fig.width=8.5, fig.height=5.25, dev='svg', cache=F,
warning=F, message=F, cache.path=cache.path, tidy=F)
setwd(rmd.path) # setwd to base
# some logic to help us avoid overwriting already existing md files
files.rmd <- data.frame(rmd = list.files(path = rmd.path,
full.names = T,
pattern = "\\.Rmd$",
ignore.case = T,
recursive = F), stringsAsFactors=F)
files.rmd$corresponding.md.file <- paste0(posts.path, "/", basename(gsub(pattern = "\\.Rmd$", replacement = ".md", x = files.rmd$rmd)))
files.rmd$corresponding.md.exists <- file.exists(files.rmd$corresponding.md.file)
## determining which posts to overwrite from parameters overwriteOne & overwriteAll
files.rmd$md.overwriteAll <- overwriteAll
if(is.null(overwriteOne)==F) files.rmd$md.overwriteAll[grep(overwriteOne, files.rmd[,'rmd'], ignore.case=T)] <- T
files.rmd$md.render <- F
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$corresponding.md.exists[i] == F) {
files.rmd$md.render[i] <- T
}
if ((files.rmd$corresponding.md.exists[i] == T) && (files.rmd$md.overwriteAll[i] == T)) {
files.rmd$md.render[i] <- T
}
}
# For each Rmd file, render markdown (contingent on the flags set above)
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$md.render[i] == T) {
out.file <- knit(as.character(files.rmd$rmd[i]),
output = as.character(files.rmd$corresponding.md.file[i]),
envir = parent.frame(),
quiet = T)
message(paste0("KnitPost(): ", basename(files.rmd$rmd[i])))
}
}
}
## actually using function
#KnitPost(overwriteOne='test', site.path='/Users/ajb/Documents/github/simpleblog/')
KnitPost(overwriteOne='advanced-data-table', site.path='/Users/ajb/Documents/github/simpleblog/')
rm(list=ls())
