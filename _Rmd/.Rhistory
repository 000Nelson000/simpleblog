else {print(paste0(i, ' pages collected')); break}
print(i)
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=111, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F) {
metaArt <- meta
if(overwrite==T) {artIndex <- 1:n
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') paste0(i, ' - could not extract article')
Sys.sleep(.5)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
########################################################
## this function generates the URL for a GET request to the New York Times Article Search API
########################################################
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
## examples
if(1==0){
library('httr')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20131025', key='sample-key')
}
########################################################
## this function actually makes the GET requests to the NYT Article Search API
########################################################
getMeta <- function(url, pages=Inf, sleep=0.1) {
art <- list()
i <- 1
e <- c(-3,-2,-1)
while(i<pages){
if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1
tryget <- try({
urlp <- gsub('page=\\d+', paste0('page=', i), url)
p <- GET(urlp)
pt <- content(p, 'parsed')
if(length(pt$response$docs)>0) art <- append(art, pt$response$docs)
else {print(paste0(i, ' pages collected')); break}
print(i)
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=111, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F, sleep=0.1) {
metaArt <- meta
if(overwrite==T) {artIndex <- 1:n
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
library('httr')
library('XML')
## make urls to get meta data
url <- makeURL(begin_date='20140102', end_date='20140105', key='sample-key') #, key='sample-key'
## collect meta data
a <- getMeta(url, pages=15)
## collect articles
artxt <- getArticles(a, n=20, overwrite=F)
artxt <- getArticles(artxt, n=20, overwrite=F, sleep=0)
artxt <- getArticles(artxt, n=15, overwrite=F, sleep=0)
sqrt/9238
sqrt(238)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
dbShowCollections(mg1)
query <- dbGetQuery(mg1, 'testData', "{x:3}")
## insert document
a[[2]]$body <- bb[[2]]
doc <- a[[2]]
output <- dbInsertDocument(mg1, "nyt", toJSON(doc))
query <- dbGetQuery(mg1, 'nyt', "{wordcount:128}")
query
mg1 <- mongoDbConnect('nyt')
mg2 <- mongoDbConnect('nyt')
mg2
dbShowCollections(mg2)
dbShowCollections
dbShowCollections(mg2)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
dbShowCollections(mg1)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
mongoDbConnect(host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='63240')
mg1 <- mongoDbConnect(dbName='nyt', host='63240')
dbShowCollections(mg1)
host <- "ds063240.mongolab.com:63240/"
username <- "ajb073"
password <- "andrewb03"
db <- "nyt"
mongo <- mongo.create(host=host , db=db, username=username, password=password)
library('rmongodb')
mongo <- mongo.create(host=host , db=db, username=username, password=password)
library('httr')
library('rjson')
library('httr')
library('RMongo')
library('XML')
########################################################
## this function generates the URL for a GET request to the New York Times Article Search API
########################################################
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0,
sort=NULL, fl=NULL, hl=NULL, facet_field=NULL, facet_filter=NULL){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page,
sort=sort, fl=fl, hl=hl, facet_field=facet_field, facet_filter=facet_filter)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
## examples
if(1==0){
library('httr')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20131025', key='sample-key')
}
########################################################
## this function actually makes the GET requests to the NYT Article Search API
########################################################
getMeta <- function(url, pages=Inf, sleep=0.1, tryn=3) {
art <- list()
i <- 1
e <- seq(-tryn, -tryn/2, length.out=tryn)
while(i<=pages){
if(length(unique(e[(length(e)-(tryn-1)):length(e)]))==1) i <- i+1 ## attempt tryn times before moving to next page
tryget <- try({
urlp <- gsub('page=\\d+', paste0('page=', i), url)
p <- GET(urlp)
pt <- content(p, 'parsed')
if(length(pt$response$docs)>0) art <- append(art, pt$response$docs)
else {print(paste0(i, ' pages collected')); break}
if(i %% 10 ==0) print(paste0(i, ' pages of metadata collected'))
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - metadata page not scraped'))
e <- c(e, i)
e <- e[(length(e)-(tryn-1)):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=22, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F, sleep=0.1, mongo=list(dbName, collection, host='127.0.0.1', username, password, ...)) {
metaArt <- meta
if(is.null(mongo$dbName)==F){
con <- mongoDbConnect(mongo$dbName, mongo$host)
try(authenticated <-dbAuthenticate(con, username=mongo$username, password=mongo$password))
}
if(overwrite==T) {artIndex <- 1:(min(n, length(meta)))
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']]))) ## get index of articles that have not been scraped yet
artIndex <- ii[1:min(n,length(ii))] ## take first n articles
}
e <- c(-3,-2,-1)
i<-1
while(i<=length(artIndex)){
if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1 ## if we tried and failed 3 times, move on
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
html <- content(p, 'text') ## metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(html)
if(is.null(mongo$dbName)==F) dbInsertDocument(con, mongo$collection, toJSON(metaArt[[i]]))
if(i %% 10==0) print(paste0(i, ' articles scraped'))
i<-i+1
}
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - article not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
library('rjson')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, mongo=list(dbName='nyt', collection='test2', host='ds063240.mongolab.com:63240', username='user1', password='password'))
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
url
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?facet_field=source&facet_filter=true&begin_date=20130105&end_date=20130105&api-key=sample-key'
a<- getMeta(url, pages=20)
a[[1]]
str(a[[1]])
str(a[[10]])
str(a[[11]])
str(a[[12]])
str(a[[13]])
str(a[[20]])
str(a[[21]])
str(a[[25]])
str(a[[45]])
length(a)
a[[200]]
a[[201]]
names(a)
unlist(a)
aa<-unlist(a)
names(aa)
names(aa)['facets']
names(aa)['_id']
names(aa)['snippet']
sort(names(aa))
sort(unique(names(aa)))
a<- getMeta(url, pages=1)
a[[1]]
a[[20]]
a[[2]]
aa<-GET(url)
aa
content(aa, 'parsed')
a<-content(aa, 'parsed')
a[[11]]
a[[10]]
a
a$response$facets
type(cora.documents)
class(cora.documents)
library('cora')
library('datasets')
class(cora.documents)
cora.documents
cora
?knit
library('knitr')
?knitr
knitr
knit
KnitPost <- function(site.path='/Users/abrooks/Documents/github/simpleblog/', overwriteAll=FALSE, overwriteOne=NULL) {
library('knitr')
# convert all Rmd files in _Rmd/* to markdown files
# directory of jekyll blog (including trailing slash)
site.path <- site.path
# directory where your Rmd-files reside (relative to base)
rmd.path <- paste0(site.path, "_Rmd")
# directory to save figures
fig.dir <- "assets/Rfig/"
# directory for converted markdown files
posts.path <- paste0(site.path, "_posts/articles/")
# cache
cache.path <- paste0(site.path, "_cache")
#library('knitr')
render_jekyll(highlight = "pygments")
# "base.dir is never used when composing the URL of the figures; it is
# only used to save the figures to a different directory, which can
# be useful when you do not want the figures to be saved under the
# current working directory.
# The URL of an image is always base.url + fig.path"
# https://groups.google.com/forum/#!topic/knitr/18aXpOmsumQ
opts_knit$set(base.url = '/',
base.dir = site.path)
opts_chunk$set(fig.path   = fig.dir,
fig.width  = 8.5,
fig.height = 5.25,
dev        = 'svg',
cache      = FALSE,
warning    = FALSE,
message    = FALSE,
cache.path = cache.path,
tidy       = FALSE)
# setwd to base
setwd(rmd.path)
# some logic to help us avoid overwriting already existing md files
files.rmd <-
data.frame(rmd = list.files(path = rmd.path,
full.names = TRUE,
pattern = "\\.Rmd$",
ignore.case = TRUE,
recursive = FALSE), stringsAsFactors=F)
files.rmd$corresponding.md.file <- paste0(posts.path, "/", basename(gsub(pattern = "\\.Rmd$", replacement = ".md", x = files.rmd$rmd)))
files.rmd$corresponding.md.exists <- file.exists(files.rmd$corresponding.md.file)
files.rmd$md.overwriteAll <- overwriteAll
if(is.null(overwriteOne)==F) files.rmd$md.overwriteAll[grep(overwriteOne, files.rmd[,'rmd'], ignore.case=T)] <- TRUE
files.rmd$md.render <- FALSE
for (i in 1:dim(files.rmd)[1]) {
if (files.rmd$corresponding.md.exists[i] == FALSE) {
files.rmd$md.render[i] <- TRUE
}
if ((files.rmd$corresponding.md.exists[i] == TRUE) && (files.rmd$md.overwriteAll[i] == TRUE)) {
files.rmd$md.render[i] <- TRUE
}
}
# For each Rmd file, render markdown (contingent on the flags set above)
for (i in 1:dim(files.rmd)[1]) {
# if clause to make sure we only re-knit if overwriteAll==TRUE or .md not already existing
if (files.rmd$md.render[i] == TRUE) {
# KNITTING ----
#out.file <- basename(knit(files.rmd$rmd[i], envir = parent.frame(), quiet = TRUE))
out.file <- knit(as.character(files.rmd$rmd[i]),
output = as.character(files.rmd$corresponding.md.file[i]),
envir = parent.frame(),
quiet = TRUE)
message(paste0("KnitPost(): ", basename(files.rmd$rmd[i])))
}
}
}
KnitPost(overwriteOne='test')
KnitPost(overwriteOne = 'test', overwriteAll=F)
KnitPost(overwriteOne = 'test')
KnitPost(overwriteOne = 'test')
KnitPost(overwriteOne = 'test')
KnitPost(overwriteOne = 'test')
KnitPost(overwriteOne = 'test')
2700/12
225/3
2700/12
225/3
225/160
Rsenal
library('Rsenal')
library('Rsenal')
binCat(mtcars$cyl)
?roundCut
x1 <- cut(quantile(rnorm(100)), breaks=4)
roundCut(x1, 1)
?binCat
d <- rpois(1000, 20)
d[d>26] <- sample(1:26, length(d[d>26]), replace=T)
dl <- letters[d]
barplot(table(dl))
table(binCat(dl, results=F, ncat=5))
